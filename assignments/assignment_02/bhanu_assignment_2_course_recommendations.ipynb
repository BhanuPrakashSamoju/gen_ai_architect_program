{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHUliGycPfeJLxFkESyg//",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhanuPrakashSamoju/gen_ai_architect_program/blob/main/assignments/assignment_02/bhanu_assignment_2_course_recommendations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S92_Luuo15l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InHWJF976SZA",
        "outputId": "0f90b2b7-37d5-4576-844b-f1615b947013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 2)) (0.3.27)\n",
            "Requirement already satisfied: langgraph==0.6.7 in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 3)) (0.6.7)\n",
            "Requirement already satisfied: langchain-openai==0.3.33 in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 4)) (0.3.33)\n",
            "Requirement already satisfied: langchain-community==0.3.29 in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 5)) (0.3.29)\n",
            "Requirement already satisfied: pydantic==2.11.7 in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 6)) (2.11.7)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 13)) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 14)) (2.32.5)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 17)) (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27->-r /content/requirements.txt (line 2)) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27->-r /content/requirements.txt (line 2)) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27->-r /content/requirements.txt (line 2)) (0.4.28)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27->-r /content/requirements.txt (line 2)) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27->-r /content/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7->-r /content/requirements.txt (line 3)) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7->-r /content/requirements.txt (line 3)) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7->-r /content/requirements.txt (line 3)) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7->-r /content/requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.33->-r /content/requirements.txt (line 4)) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.33->-r /content/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.7->-r /content/requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.7->-r /content/requirements.txt (line 6)) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.7->-r /content/requirements.txt (line 6)) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.7->-r /content/requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/requirements.txt (line 10)) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (1.75.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (0.17.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r /content/requirements.txt (line 10)) (4.25.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/requirements.txt (line 13)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/requirements.txt (line 13)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/requirements.txt (line 13)) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/requirements.txt (line 14)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/requirements.txt (line 14)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/requirements.txt (line 14)) (2025.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (1.20.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb->-r /content/requirements.txt (line 10)) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb->-r /content/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb->-r /content/requirements.txt (line 10)) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb->-r /content/requirements.txt (line 10)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb->-r /content/requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb->-r /content/requirements.txt (line 10)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb->-r /content/requirements.txt (line 10)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb->-r /content/requirements.txt (line 10)) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (3.3.1)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27->-r /content/requirements.txt (line 2)) (1.33)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.6.7->-r /content/requirements.txt (line 3)) (1.10.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27->-r /content/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27->-r /content/requirements.txt (line 2)) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/requirements.txt (line 10)) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/requirements.txt (line 10)) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/requirements.txt (line 10)) (1.13.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33->-r /content/requirements.txt (line 4)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33->-r /content/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33->-r /content/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb->-r /content/requirements.txt (line 10)) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/requirements.txt (line 10)) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/requirements.txt (line 10)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/requirements.txt (line 10)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r /content/requirements.txt (line 10)) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r /content/requirements.txt (line 10)) (2.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb->-r /content/requirements.txt (line 10)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb->-r /content/requirements.txt (line 10)) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27->-r /content/requirements.txt (line 2)) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.33->-r /content/requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb->-r /content/requirements.txt (line 10)) (0.35.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb->-r /content/requirements.txt (line 10)) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb->-r /content/requirements.txt (line 10)) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/requirements.txt (line 10)) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/requirements.txt (line 10)) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r /content/requirements.txt (line 10)) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r /content/requirements.txt (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r /content/requirements.txt (line 10)) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r /content/requirements.txt (line 10)) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27->-r /content/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r /content/requirements.txt (line 10)) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community==0.3.29->-r /content/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r /content/requirements.txt (line 10)) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r /content/requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/requirements.txt (line 10)) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Environment Variables for Azure Credentials ---\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/env\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj9VPl3V4o6r",
        "outputId": "a871c1cb-ca30-4b78-bf16-82e882049828"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. SETUP AND DEPENDENCIES ---\n",
        "# Ensure all packages are installed\n",
        "# !pip install langchain langgraph langchain_openai chromadb pandas python-dotenv requests beautifulsoup4\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from typing import List, Tuple, TypedDict, Optional\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END"
      ],
      "metadata": {
        "id": "Xz9QSSDK4_Je"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. DATA LOADING AND PREPARATION ---\n",
        "print(\"--- 2. Loading and Preparing Data ---\")\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/Bluedata-Consulting/GAAPB01-training-code-base/refs/heads/main/Assignments/assignment2dataset.csv\"\n",
        "\n",
        "try:\n",
        "    courses_df = pd.read_csv(DATASET_URL)\n",
        "    # print(courses_df)\n",
        "    courses_df['combined_text'] = courses_df['title'] + \": \" + courses_df['description']\n",
        "    courses_df.dropna(subset=['course_id', 'combined_text'], inplace=True)\n",
        "    print(f\"Successfully loaded and prepared {len(courses_df)} courses.\")\n",
        "    # Create a string representation of the course catalog for the LLM prompt\n",
        "    course_catalog_str = \"\\n\".join([f\"- {row['course_id']}: {row['title']}\" for _, row in courses_df.iterrows()])\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data: {e}\")\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsN6e24xBWu7",
        "outputId": "af69ff45-38b9-4869-8dbb-0583bd65a0ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. Loading and Preparing Data ---\n",
            "Successfully loaded and prepared 25 courses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "course_catalog_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rl_MN7NFUhNU",
        "outputId": "6e3c1663-a47d-422c-9e6e-acc7d9f4c794"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- C001: Foundations of Machine Learning\\n- C002: Deep Learning with TensorFlow and Keras\\n- C003: Natural Language Processing Fundamentals\\n- C004: Computer Vision and Image Processing\\n- C005: Reinforcement Learning Basics\\n- C006: Data Engineering on AWS\\n- C007: Cloud Computing with Azure\\n- C008: DevOps Practices and CI/CD\\n- C009: Containerization with Docker and Kubernetes\\n- C010: APIs and Microservices Architecture\\n- C011: Big Data Analytics with Spark\\n- C012: SQL for Data Analysis\\n- C013: NoSQL Databases and MongoDB\\n- C014: Data Visualization with Tableau\\n- C015: Business Intelligence with Power BI\\n- C016: Python Programming for Data Science\\n- C017: R Programming and Statistical Analysis\\n- C018: Product Management Essentials\\n- C019: Agile and Scrum Mastery\\n- C020: User Experience (UX) Design Principles\\n- C021: Cybersecurity Fundamentals\\n- C022: Internet of Things (IoT) Development\\n- C023: Blockchain Technology and Smart Contracts\\n- C024: Augmented and Virtual Reality Development\\n- C025: MLOps: Productionizing Machine Learning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(courses_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwbcBza_5usB",
        "outputId": "1ecb53cb-29d6-4204-f27c-4ad0aeb134ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  course_id                                     title  \\\n",
            "0      C001           Foundations of Machine Learning   \n",
            "1      C002   Deep Learning with TensorFlow and Keras   \n",
            "2      C003  Natural Language Processing Fundamentals   \n",
            "3      C004      Computer Vision and Image Processing   \n",
            "4      C005             Reinforcement Learning Basics   \n",
            "\n",
            "                                         description  \\\n",
            "0  Understand foundational machine learning algor...   \n",
            "1  Explore neural network architectures using Ten...   \n",
            "2  Dive into NLP techniques for processing and un...   \n",
            "3  Learn the principles of computer vision and im...   \n",
            "4  Get introduced to reinforcement learning parad...   \n",
            "\n",
            "                                       combined_text  \n",
            "0  Foundations of Machine Learning: Understand fo...  \n",
            "1  Deep Learning with TensorFlow and Keras: Explo...  \n",
            "2  Natural Language Processing Fundamentals: Dive...  \n",
            "3  Computer Vision and Image Processing: Learn th...  \n",
            "4  Reinforcement Learning Basics: Get introduced ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. INITIALIZE MODELS AND VECTOR STORES ---\n",
        "print(\"\\n--- 3. Initializing Models and Vector Stores ---\")\n",
        "os.environ.pop(\"OPENAI_API_BASE\", None)\n",
        "\n",
        "# --- Your existing code (with the 'model' parameter uncommented) ---\n",
        "embedding_model_name = \"text-embedding-3-small\"\n",
        "\n",
        "# Initialize Azure OpenAI Embedding Model\n",
        "embedding_model = AzureOpenAIEmbeddings(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"],\n",
        "    # It's good practice to still specify the model name\n",
        "    model=embedding_model_name,\n",
        "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"], # Make sure this env var is set\n",
        "    api_key=os.environ[\"AZURE_OPENAI_EMBEDDING_KEY\"],\n",
        "    api_version=os.environ[\"AZURE_OPENAI_EMBEDDING_API_VERSION\"],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck8OWGGL6bAx",
        "outputId": "0773873f-3f89-47e6-cd5b-081f5a6a645a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. Initializing Models and Vector Stores ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_model.embed_query(\"Hi how are you?\")"
      ],
      "metadata": {
        "id": "O2OE8XtIE4jB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Azure OpenAI Chat Model (GPT-4o mini)\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
        "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
        "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    openai_api_type=os.getenv(\"OPENAI_API_TYPE\"),\n",
        "    temperature=0.2,\n",
        "    # streaming=False,\n",
        ")"
      ],
      "metadata": {
        "id": "vesQkzjY9sJR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3V6u8rMHlr7",
        "outputId": "01f551f9-7275-4bd0-e07e-41f0733fded5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CKpVWIJDnCwV80o2NpIEYD4NaxXlI', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--d201f591-4c30-490e-b6ba-56e1345bc65f-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- Vector Store for COURSES ---\n",
        "print(\"Initializing course catalog vector store...\")\n",
        "course_documents = [\n",
        "    Document(\n",
        "        page_content=row['combined_text'],\n",
        "        metadata={'course_id': row['course_id'], 'title': row['title']}\n",
        "    ) for _, row in courses_df.iterrows()\n",
        "]\n",
        "course_vector_store = Chroma.from_documents(\n",
        "    documents=course_documents,\n",
        "    embedding=embedding_model,\n",
        "    collection_name=\"courses\"\n",
        ")\n",
        "course_retriever = course_vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# --- Vector Store for USER PREFERENCES ---\n",
        "print(\"Initializing user preference vector store...\")\n",
        "# This DB will store extracted user interests over time\n",
        "preference_vector_store = Chroma(\n",
        "    collection_name=\"user_preferences\",\n",
        "    embedding_function=embedding_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP2eVcPb49j2",
        "outputId": "671e9d4b-88b9-4b80-f02b-1807db3401ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing course catalog vector store...\n",
            "Initializing user preference vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-988557995.py:19: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  preference_vector_store = Chroma(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. BUILDING THE ENHANCED RECOMMENDATION GRAPH ---\n",
        "print(\"\\n--- 4. Building the Enhanced LangGraph Recommendation Graph ---\")\n",
        "\n",
        "# Pydantic model for structured output from the LLM\n",
        "class RecognizedPreferences(BaseModel):\n",
        "    preferences: List[str] = Field(description=\"A list of key skills, topics, or technologies the user is interested in.\")\n",
        "\n",
        "class CompletedCourses(BaseModel):\n",
        "    completed_course_ids: List[str] = Field(description=\"A list of course_ids that the user has completed based on the query.\")\n",
        "\n",
        "# Define the state for the graph\n",
        "class RecommendationState(TypedDict):\n",
        "    user_id: str\n",
        "    profile: str\n",
        "    completed_ids: List[str]\n",
        "    historical_preferences: Optional[List[str]]\n",
        "    synthesized_query: str\n",
        "    final_recommendations: List[Tuple[str, float]]\n",
        "\n",
        "# Define the nodes of the graph\n",
        "def extract_completed_courses_node(state: RecommendationState) -> RecommendationState:\n",
        "    \"\"\"Uses an LLM to extract completed course IDs from the user's natural language query.\"\"\"\n",
        "    print(\">> Node: extract_completed_courses_node\")\n",
        "    profile = state[\"profile\"]\n",
        "\n",
        "    structured_llm = llm.with_structured_output(CompletedCourses)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are an expert at reading a user query and identifying which courses they have completed from a provided catalog. Only return IDs for courses explicitly mentioned as completed. If none are mentioned, return an empty list.\"),\n",
        "        (\"human\", \"From the user's query below, identify the `course_id` for any courses they have completed.\\n\\n## Course Catalog:\\n{catalog}\\n\\n## User Query:\\n'{profile}'\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | structured_llm\n",
        "    result = chain.invoke({\"profile\": profile, \"catalog\": course_catalog_str})\n",
        "\n",
        "    completed_ids = result.completed_course_ids\n",
        "    print(f\"   LLM extracted completed courses: {completed_ids}\")\n",
        "\n",
        "    return {**state, \"completed_ids\": completed_ids}\n",
        "\n",
        "# Define the nodes of the graph\n",
        "def fetch_user_history_node(state: RecommendationState) -> RecommendationState:\n",
        "    \"\"\"Fetches the user's past preferences from the preference vector store.\"\"\"\n",
        "    print(\">> Node: fetch_user_history_node\")\n",
        "    user_id = state[\"user_id\"]\n",
        "    # In Chroma, we can filter by metadata to get user-specific documents\n",
        "    try:\n",
        "        # A simple way to get all docs for a user is to do a broad search and filter\n",
        "        # For larger scale, a proper metadata query would be better.\n",
        "        results = preference_vector_store.get(where={\"user_id\": user_id}, include=[\"documents\"])\n",
        "        history = [doc for doc in results['documents']] if results else []\n",
        "        print(f\"   Found {len(history)} historical preferences for user '{user_id}'.\")\n",
        "        return {**state, \"historical_preferences\": history}\n",
        "    except Exception as e:\n",
        "        print(f\"   Could not retrieve history for user '{user_id}': {e}\")\n",
        "        return {**state, \"historical_preferences\": []}\n",
        "\n",
        "\n",
        "def synthesize_and_recognize_node(state: RecommendationState) -> RecommendationState:\n",
        "    \"\"\"\n",
        "    1. Recognizes preferences from the current query and indexes them.\n",
        "    2. Synthesizes a new query using the current request AND historical data.\n",
        "    \"\"\"\n",
        "    print(\">> Node: synthesize_and_recognize_node\")\n",
        "    user_id = state[\"user_id\"]\n",
        "    profile = state[\"profile\"]\n",
        "    history = state.get(\"historical_preferences\", [])\n",
        "\n",
        "    # Part 1: Recognize and Index Preferences from CURRENT query using .with_structured_output\n",
        "\n",
        "    # Create an LLM instance that is bound to the Pydantic output structure\n",
        "    structured_llm = llm.with_structured_output(RecognizedPreferences)\n",
        "\n",
        "    # Create a simpler prompt, as formatting instructions are handled automatically\n",
        "    recognition_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are an expert at extracting key skills, topics, and technologies from user queries.\"),\n",
        "        (\"human\", \"Extract the key topics from the following user query: '{profile}'\")\n",
        "    ])\n",
        "\n",
        "    # Create and invoke the chain\n",
        "    chain = recognition_prompt | structured_llm\n",
        "    recognized = chain.invoke({\"profile\": profile})\n",
        "\n",
        "    new_preferences = recognized.preferences\n",
        "    if new_preferences:\n",
        "        print(f\"   Recognized new preferences: {new_preferences}\")\n",
        "        # Create and add documents to the preference store\n",
        "        pref_docs = [Document(page_content=pref, metadata={\"user_id\": user_id}) for pref in new_preferences]\n",
        "        preference_vector_store.add_documents(pref_docs)\n",
        "        print(f\"   Indexed {len(pref_docs)} new preferences for user '{user_id}'.\")\n",
        "\n",
        "    # Part 2: Synthesize a search query using current AND historical data\n",
        "    history_str = \", \".join(history) if history else \"None\"\n",
        "    synthesis_prompt = f\"\"\"\n",
        "    A user has provided the following current interest: \"{profile}\"\n",
        "\n",
        "    Based on their history, they have also previously shown interest in: {history_str}.\n",
        "\n",
        "    Synthesize a concise, single paragraph that combines their current interest with their historical preferences to describe the ideal topics for their next course. Focus on keywords and concepts for a semantic search.\n",
        "    \"\"\"\n",
        "    response = llm.invoke(synthesis_prompt)\n",
        "    synthesized_query = response.content\n",
        "    print(f\"   Synthesized Query (with history): {synthesized_query}\")\n",
        "\n",
        "    return {**state, \"synthesized_query\": synthesized_query}\n",
        "\n",
        "def retrieve_courses_node(state: RecommendationState) -> RecommendationState:\n",
        "    \"\"\"Retrieves similar courses from the course catalog using the synthesized query.\"\"\"\n",
        "    print(\">> Node: retrieve_courses_node\")\n",
        "    query = state[\"synthesized_query\"]\n",
        "    docs_with_scores = course_vector_store.similarity_search_with_relevance_scores(query, k=10)\n",
        "\n",
        "    # Filter and rank\n",
        "    completed_ids = state[\"completed_ids\"]\n",
        "    recommendations = []\n",
        "    for doc, score in docs_with_scores:\n",
        "        course_id = doc.metadata.get('course_id')\n",
        "        if course_id not in completed_ids:\n",
        "            recommendations.append((course_id, score))\n",
        "\n",
        "    top_5 = sorted(recommendations, key=lambda x: x[1], reverse=True)[:5]\n",
        "    print(f\"   Retrieved and filtered top {len(top_5)} recommendations.\")\n",
        "    return {**state, \"final_recommendations\": top_5}\n",
        "\n",
        "# Define and compile the new graph structure\n",
        "workflow = StateGraph(RecommendationState)\n",
        "workflow.add_node(\"extract_completed_courses\", extract_completed_courses_node)\n",
        "workflow.add_node(\"fetch_user_history\", fetch_user_history_node)\n",
        "workflow.add_node(\"synthesize_and_recognize\", synthesize_and_recognize_node)\n",
        "workflow.add_node(\"retrieve_and_rank\", retrieve_courses_node)\n",
        "\n",
        "workflow.set_entry_point(\"extract_completed_courses\")\n",
        "workflow.add_edge(\"extract_completed_courses\", \"fetch_user_history\")\n",
        "workflow.add_edge(\"fetch_user_history\", \"synthesize_and_recognize\")\n",
        "workflow.add_edge(\"synthesize_and_recognize\", \"retrieve_and_rank\")\n",
        "workflow.add_edge(\"retrieve_and_rank\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"Enhanced LangGraph application compiled successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t04-DDUF5NEl",
        "outputId": "1c5b1e03-9c77-4e24-b06d-f9cd2634b1fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 4. Building the Enhanced LangGraph Recommendation Graph ---\n",
            "Enhanced LangGraph application compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. THE MAIN RECOMMENDATION FUNCTION (SIMPLIFIED) ---\n",
        "def recommend_courses(user_id: str, profile: str) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Runs the full recommendation pipeline, including LLM-based extraction of completed courses.\"\"\"\n",
        "    inputs = {\n",
        "        \"user_id\": user_id,\n",
        "        \"profile\": profile,\n",
        "    }\n",
        "    final_state = app.invoke(inputs)\n",
        "    return final_state.get(\"final_recommendations\", [])"
      ],
      "metadata": {
        "id": "UHCgAdUg5QKk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. MODULAR EVALUATION REPORT ---\n",
        "\n",
        "def run_evaluation_query(user_id: str, query: str):\n",
        "    \"\"\"\n",
        "    A modular function to run a single evaluation query through the pipeline and print results.\n",
        "    Includes an LLM evaluation of the top recommendations.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"EVALUATING QUERY FOR USER '{user_id}'\")\n",
        "    print(f\"User Query: {query}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Get recommendations (completed courses are extracted automatically)\n",
        "    recommendations = recommend_courses(user_id, query)\n",
        "\n",
        "    print(\"\\nTop 5 Recommendations:\")\n",
        "    if recommendations:\n",
        "        for course_id, score in recommendations:\n",
        "            course_details = courses_df[courses_df['course_id'] == course_id].iloc[0]\n",
        "            print(f\"  - [ID: {course_id}] {course_details['title']} (Score: {score:.4f})\")\n",
        "    else:\n",
        "        print(\"  No recommendations found.\")\n",
        "\n",
        "    # --- LLM Evaluation of Recommendations ---\n",
        "    print(\"\\n--- LLM Evaluation of Recommendations ---\")\n",
        "    if recommendations:\n",
        "        recommendation_str = \"\\n\".join([f\"- [ID: {cid}] {courses_df[courses_df['course_id'] == cid].iloc[0]['title']}\" for cid, score in recommendations])\n",
        "\n",
        "        evaluation_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are an expert in course recommendations. Based on the user's query and the provided course catalog, evaluate if the given top 5 recommendations are relevant and appropriate. Provide a brief explanation for your assessment.\"),\n",
        "            (\"human\", \"## Course Catalog:\\n{catalog}\\n\\n## User Query:\\n'{query}'\\n\\n## Top 5 Recommendations:\\n{recommendations}\\n\\nEvaluate the relevance of these recommendations:\")\n",
        "        ])\n",
        "\n",
        "        evaluation_chain = evaluation_prompt | llm\n",
        "        evaluation_result = evaluation_chain.invoke({\"catalog\": course_catalog_str, \"query\": query, \"recommendations\": recommendation_str})\n",
        "\n",
        "        print(evaluation_result.content)\n",
        "    else:\n",
        "        print(\"No recommendations to evaluate.\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "print(\"\\n--- 6. Evaluation Report ---\")\n",
        "\n",
        "\n",
        "evaluation_queries = [\n",
        "    {\n",
        "        \"user_id\" : \"user_1\",\n",
        "        \"query\": \"I've completed the 'Python Programming for Data Science' course and enjoy data visualization. What should I take next?\"\n",
        "    },\n",
        "    {\n",
        "        \"user_id\" : \"user_2\",\n",
        "        \"query\": \"I know Azure basics and want to manage containers and build CI/CD pipelines. Recommend courses.\",\n",
        "    },\n",
        "    {\n",
        "        \"user_id\" : \"user_3\",\n",
        "        \"query\": \"My background is in ML fundamentals; I'd like to specialize in neural networks and production workflows.\",\n",
        "    },\n",
        "    {\n",
        "        \"user_id\" : \"user_2\",\n",
        "        \"query\": \"I want to learn to build and deploy microservices with Kubernetes—what courses fit best?\",\n",
        "    },\n",
        "    {\n",
        "        \"user_id\" : \"user_4\",\n",
        "        \"query\": \"I'm interested in blockchain and smart contracts but have no prior experience. Which courses do you suggest?\",\n",
        "    }\n",
        "]\n",
        "\n",
        "for item in evaluation_queries:\n",
        "    run_evaluation_query(\n",
        "        user_id=item[\"user_id\"],\n",
        "        query=item[\"query\"],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY--ZshU5Vez",
        "outputId": "13bc4cfe-d6ce-47e6-90af-2fa0cbeeec4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 6. Evaluation Report ---\n",
            "\n",
            "==================================================\n",
            "EVALUATING QUERY FOR USER 'user_1'\n",
            "User Query: I've completed the 'Python Programming for Data Science' course and enjoy data visualization. What should I take next?\n",
            "==================================================\n",
            ">> Node: extract_completed_courses_node\n",
            "   LLM extracted completed courses: ['C016']\n",
            ">> Node: fetch_user_history_node\n",
            "   Found 4 historical preferences for user 'user_1'.\n",
            ">> Node: synthesize_and_recognize_node\n",
            "   Recognized new preferences: ['Python Programming', 'Data Science', 'Data Visualization', 'Course Recommendations']\n",
            "   Indexed 4 new preferences for user 'user_1'.\n",
            "   Synthesized Query (with history): Given your completion of the \"Python Programming for Data Science\" course and your enthusiasm for data visualization, the ideal next step would be to explore advanced data visualization techniques using Python libraries such as Matplotlib, Seaborn, or Plotly. Additionally, consider courses that delve into data storytelling, interactive dashboards, or machine learning applications in data visualization. This will not only enhance your skills in data science but also allow you to effectively communicate insights through compelling visual narratives.\n",
            ">> Node: retrieve_courses_node\n",
            "   Retrieved and filtered top 5 recommendations.\n",
            "\n",
            "Top 5 Recommendations:\n",
            "  - [ID: C014] Data Visualization with Tableau (Score: 0.3821)\n",
            "  - [ID: C011] Big Data Analytics with Spark (Score: 0.2402)\n",
            "  - [ID: C017] R Programming and Statistical Analysis (Score: 0.2098)\n",
            "  - [ID: C015] Business Intelligence with Power BI (Score: 0.2013)\n",
            "  - [ID: C006] Data Engineering on AWS (Score: 0.1860)\n",
            "\n",
            "--- LLM Evaluation of Recommendations ---\n",
            "The top 5 recommendations provided for the user who has completed the 'Python Programming for Data Science' course and enjoys data visualization are as follows:\n",
            "\n",
            "1. **Data Visualization with Tableau (C014)**: Highly relevant. Since the user has expressed an interest in data visualization, this course will build on that interest and provide them with practical skills in a popular visualization tool.\n",
            "\n",
            "2. **Big Data Analytics with Spark (C011)**: Relevant. While this course focuses more on big data processing, it can complement the user's data science skills and potentially enhance their data visualization capabilities by working with larger datasets.\n",
            "\n",
            "3. **R Programming and Statistical Analysis (C017)**: Somewhat relevant. Although R is a powerful tool for data analysis and visualization, the user has already completed a Python course. This course may not directly align with their current focus on data visualization, but it could still be beneficial for statistical analysis.\n",
            "\n",
            "4. **Business Intelligence with Power BI (C015)**: Highly relevant. Similar to the Tableau course, Power BI is another widely used tool for data visualization and business intelligence. This would be a great next step for someone interested in visualizing data effectively.\n",
            "\n",
            "5. **Data Engineering on AWS (C006)**: Less relevant. While data engineering is an important aspect of data science, this course does not directly relate to data visualization. It may not align with the user's expressed interest in visualization.\n",
            "\n",
            "Overall, the recommendations are mostly relevant, particularly the courses focused on data visualization (C014 and C015). The inclusion of C011 and C017 can provide additional skills, but C006 is less aligned with the user's stated interests.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EVALUATING QUERY FOR USER 'user_2'\n",
            "User Query: I know Azure basics and want to manage containers and build CI/CD pipelines. Recommend courses.\n",
            "==================================================\n",
            ">> Node: extract_completed_courses_node\n",
            "   LLM extracted completed courses: []\n",
            ">> Node: fetch_user_history_node\n",
            "   Found 10 historical preferences for user 'user_2'.\n",
            ">> Node: synthesize_and_recognize_node\n",
            "   Recognized new preferences: ['Azure', 'Containers', 'CI/CD Pipelines', 'Course Recommendations']\n",
            "   Indexed 4 new preferences for user 'user_2'.\n",
            "   Synthesized Query (with history): The ideal course for the user would focus on advanced Azure container management, integrating CI/CD pipelines specifically for microservices architecture. It should cover Kubernetes orchestration for deploying and scaling microservices, along with best practices for building and managing containerized applications in Azure. Additionally, the course should include hands-on labs for practical experience in setting up CI/CD workflows, leveraging Azure DevOps, and utilizing container registries, ensuring a comprehensive understanding of modern cloud-native development and deployment strategies.\n",
            ">> Node: retrieve_courses_node\n",
            "   Retrieved and filtered top 5 recommendations.\n",
            "\n",
            "Top 5 Recommendations:\n",
            "  - [ID: C007] Cloud Computing with Azure (Score: 0.4430)\n",
            "  - [ID: C009] Containerization with Docker and Kubernetes (Score: 0.4213)\n",
            "  - [ID: C008] DevOps Practices and CI/CD (Score: 0.3488)\n",
            "  - [ID: C025] MLOps: Productionizing Machine Learning (Score: 0.1819)\n",
            "  - [ID: C019] Agile and Scrum Mastery (Score: 0.1618)\n",
            "\n",
            "--- LLM Evaluation of Recommendations ---\n",
            "The top 5 recommendations provided are largely relevant and appropriate for the user's query. Here's a brief assessment of each course:\n",
            "\n",
            "1. **C007: Cloud Computing with Azure** - This course is relevant as the user already knows Azure basics. It will deepen their understanding of Azure, which is beneficial for managing containers and building CI/CD pipelines in that environment.\n",
            "\n",
            "2. **C009: Containerization with Docker and Kubernetes** - This course is highly relevant as it directly addresses the user's interest in managing containers. Docker and Kubernetes are essential tools for containerization, making this a critical recommendation.\n",
            "\n",
            "3. **C008: DevOps Practices and CI/CD** - This course is also very relevant since the user wants to build CI/CD pipelines. Understanding DevOps practices will provide the necessary knowledge and skills to implement CI/CD effectively.\n",
            "\n",
            "4. **C025: MLOps: Productionizing Machine Learning** - While this course may not be directly related to the user's immediate focus on containers and CI/CD, it could still be relevant if the user is interested in deploying machine learning models in a production environment. However, it might not be as high a priority compared to the other recommendations.\n",
            "\n",
            "5. **C019: Agile and Scrum Mastery** - This course is less relevant to the user's specific request about managing containers and building CI/CD pipelines. While Agile and Scrum methodologies can be beneficial in a DevOps context, they do not directly address the technical skills the user is seeking.\n",
            "\n",
            "Overall, the first three courses (C007, C009, and C008) are highly relevant and directly aligned with the user's goals. Course C025 is somewhat relevant but not a top priority, while C019 is the least relevant in the context of the user's specific query.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EVALUATING QUERY FOR USER 'user_3'\n",
            "User Query: My background is in ML fundamentals; I'd like to specialize in neural networks and production workflows.\n",
            "==================================================\n",
            ">> Node: extract_completed_courses_node\n",
            "   LLM extracted completed courses: []\n",
            ">> Node: fetch_user_history_node\n",
            "   Found 3 historical preferences for user 'user_3'.\n",
            ">> Node: synthesize_and_recognize_node\n",
            "   Recognized new preferences: ['Machine Learning (ML) fundamentals', 'Neural Networks', 'Production Workflows']\n",
            "   Indexed 3 new preferences for user 'user_3'.\n",
            "   Synthesized Query (with history): The ideal course for the user would encompass advanced topics in neural networks, emphasizing architectures such as convolutional and recurrent networks, along with practical applications in production workflows. It should cover model deployment strategies, scalability, and monitoring in real-world environments, integrating concepts from machine learning fundamentals. Additionally, the course could explore optimization techniques, transfer learning, and best practices for maintaining and updating neural network models in production, ensuring a comprehensive understanding of both theoretical and practical aspects of machine learning in industry settings.\n",
            ">> Node: retrieve_courses_node\n",
            "   Retrieved and filtered top 5 recommendations.\n",
            "\n",
            "Top 5 Recommendations:\n",
            "  - [ID: C002] Deep Learning with TensorFlow and Keras (Score: 0.3933)\n",
            "  - [ID: C025] MLOps: Productionizing Machine Learning (Score: 0.3801)\n",
            "  - [ID: C001] Foundations of Machine Learning (Score: 0.3599)\n",
            "  - [ID: C004] Computer Vision and Image Processing (Score: 0.2564)\n",
            "  - [ID: C003] Natural Language Processing Fundamentals (Score: 0.2188)\n",
            "\n",
            "--- LLM Evaluation of Recommendations ---\n",
            "The top 5 recommendations provided are mostly relevant to the user's background and interests. Here's an assessment of each course:\n",
            "\n",
            "1. **C002: Deep Learning with TensorFlow and Keras** - Highly relevant. This course directly aligns with the user's desire to specialize in neural networks, as TensorFlow and Keras are popular frameworks for building deep learning models.\n",
            "\n",
            "2. **C025: MLOps: Productionizing Machine Learning** - Highly relevant. This course addresses the user's interest in production workflows, focusing on how to deploy and manage machine learning models in a production environment.\n",
            "\n",
            "3. **C001: Foundations of Machine Learning** - Somewhat relevant. While this course provides essential knowledge, the user already has a background in ML fundamentals. They may find this course less beneficial compared to more advanced options.\n",
            "\n",
            "4. **C004: Computer Vision and Image Processing** - Somewhat relevant. This course could be of interest if the user wants to apply neural networks specifically in the field of computer vision. However, it may not directly address their focus on general neural networks or production workflows.\n",
            "\n",
            "5. **C003: Natural Language Processing Fundamentals** - Somewhat relevant. Similar to the computer vision course, this could be interesting if the user is looking to apply neural networks in NLP. However, it may not be as aligned with their primary goal of specializing in neural networks and production workflows.\n",
            "\n",
            "Overall, the top 5 recommendations include strong options that align well with the user's goals, particularly C002 and C025. However, the inclusion of C001, C004, and C003 may not be as necessary given the user's existing knowledge and specific focus.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EVALUATING QUERY FOR USER 'user_2'\n",
            "User Query: I want to learn to build and deploy microservices with Kubernetes—what courses fit best?\n",
            "==================================================\n",
            ">> Node: extract_completed_courses_node\n",
            "   LLM extracted completed courses: []\n",
            ">> Node: fetch_user_history_node\n",
            "   Found 14 historical preferences for user 'user_2'.\n",
            ">> Node: synthesize_and_recognize_node\n",
            "   Recognized new preferences: ['microservices', 'Kubernetes', 'building microservices', 'deploying microservices', 'online courses', 'learning resources']\n",
            "   Indexed 6 new preferences for user 'user_2'.\n",
            "   Synthesized Query (with history): The ideal course for the user would focus on building and deploying microservices using Kubernetes, with an emphasis on Azure integration, containerization techniques, and CI/CD pipeline implementation. The curriculum should cover best practices for microservices architecture, hands-on experience with Kubernetes orchestration, and strategies for efficient deployment and scaling of containerized applications. Additionally, the course should include practical exercises and real-world scenarios to reinforce learning and provide insights into the latest tools and technologies in the microservices ecosystem.\n",
            ">> Node: retrieve_courses_node\n",
            "   Retrieved and filtered top 5 recommendations.\n",
            "\n",
            "Top 5 Recommendations:\n",
            "  - [ID: C009] Containerization with Docker and Kubernetes (Score: 0.4664)\n",
            "  - [ID: C007] Cloud Computing with Azure (Score: 0.4153)\n",
            "  - [ID: C008] DevOps Practices and CI/CD (Score: 0.2745)\n",
            "  - [ID: C010] APIs and Microservices Architecture (Score: 0.2169)\n",
            "  - [ID: C019] Agile and Scrum Mastery (Score: 0.1860)\n",
            "\n",
            "--- LLM Evaluation of Recommendations ---\n",
            "The top 5 recommendations for the user's query about learning to build and deploy microservices with Kubernetes are mostly relevant, but there are some nuances to consider:\n",
            "\n",
            "1. **C009: Containerization with Docker and Kubernetes** - Highly relevant. This course directly addresses the user's interest in Kubernetes, which is essential for deploying microservices. Understanding containerization is fundamental to working with Kubernetes.\n",
            "\n",
            "2. **C007: Cloud Computing with Azure** - Moderately relevant. While cloud computing is important for deploying applications, this course may not specifically focus on microservices or Kubernetes. It could provide useful context for deploying microservices in a cloud environment, but it is not directly aligned with the user's primary goal.\n",
            "\n",
            "3. **C008: DevOps Practices and CI/CD** - Relevant. This course is pertinent as it covers practices that are essential for deploying microservices effectively. Understanding CI/CD pipelines is crucial for automating the deployment of microservices, which aligns well with the user's objectives.\n",
            "\n",
            "4. **C010: APIs and Microservices Architecture** - Highly relevant. This course is directly aligned with the user's interest in microservices. It will provide foundational knowledge about designing and implementing microservices, which is critical for the user's learning path.\n",
            "\n",
            "5. **C019: Agile and Scrum Mastery** - Less relevant. While Agile and Scrum methodologies can be beneficial for managing software projects, they do not directly contribute to the technical skills needed for building and deploying microservices with Kubernetes. This course may not be necessary for the user's specific goal.\n",
            "\n",
            "### Summary:\n",
            "Overall, the recommendations include a good mix of relevant courses, particularly C009, C010, and C008, which are directly applicable to the user's interest in microservices and Kubernetes. However, C007 and C019 could be considered less critical for the immediate goal of learning to build and deploy microservices.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EVALUATING QUERY FOR USER 'user_4'\n",
            "User Query: I'm interested in blockchain and smart contracts but have no prior experience. Which courses do you suggest?\n",
            "==================================================\n",
            ">> Node: extract_completed_courses_node\n",
            "   LLM extracted completed courses: []\n",
            ">> Node: fetch_user_history_node\n",
            "   Found 5 historical preferences for user 'user_4'.\n",
            ">> Node: synthesize_and_recognize_node\n",
            "   Recognized new preferences: ['blockchain', 'smart contracts', 'online courses', 'beginner education', 'technology learning']\n",
            "   Indexed 5 new preferences for user 'user_4'.\n",
            "   Synthesized Query (with history): Given your interest in blockchain and smart contracts, along with your previous focus on beginner education and technology, an ideal course for you would cover foundational concepts of blockchain technology, the principles and functionalities of smart contracts, and practical applications in various industries. Look for courses that emphasize hands-on learning, real-world case studies, and interactive elements to enhance your understanding of decentralized systems, cryptographic security, and the development of smart contracts using popular platforms like Ethereum. Additionally, consider courses that provide a comprehensive overview of the blockchain ecosystem, including its potential impact on business and innovation.\n",
            ">> Node: retrieve_courses_node\n",
            "   Retrieved and filtered top 5 recommendations.\n",
            "\n",
            "Top 5 Recommendations:\n",
            "  - [ID: C023] Blockchain Technology and Smart Contracts (Score: 0.5229)\n",
            "  - [ID: C024] Augmented and Virtual Reality Development (Score: 0.1364)\n",
            "  - [ID: C021] Cybersecurity Fundamentals (Score: 0.1346)\n",
            "  - [ID: C001] Foundations of Machine Learning (Score: 0.1158)\n",
            "  - [ID: C022] Internet of Things (IoT) Development (Score: 0.1069)\n",
            "\n",
            "--- LLM Evaluation of Recommendations ---\n",
            "The top 5 recommendations provided for the user's interest in blockchain and smart contracts include:\n",
            "\n",
            "1. **C023: Blockchain Technology and Smart Contracts** - This is highly relevant as it directly addresses the user's interest in blockchain and smart contracts. It is the most appropriate recommendation.\n",
            "\n",
            "2. **C024: Augmented and Virtual Reality Development** - This course is not relevant to the user's query. Augmented and virtual reality are unrelated to blockchain technology and smart contracts.\n",
            "\n",
            "3. **C021: Cybersecurity Fundamentals** - While cybersecurity is an important aspect of blockchain technology, this course does not specifically focus on blockchain or smart contracts. It may provide some foundational knowledge, but it is not directly aligned with the user's interest.\n",
            "\n",
            "4. **C001: Foundations of Machine Learning** - This course is also not relevant to the user's specific interest in blockchain and smart contracts. Machine learning is a separate field and does not directly relate to the user's query.\n",
            "\n",
            "5. **C022: Internet of Things (IoT) Development** - Similar to the previous courses, IoT development does not have a direct connection to blockchain or smart contracts. While there may be intersections between IoT and blockchain, this course does not specifically cater to the user's stated interest.\n",
            "\n",
            "### Summary of Assessment:\n",
            "Only **C023: Blockchain Technology and Smart Contracts** is a relevant and appropriate recommendation for the user's interest. The other courses (C024, C021, C001, C022) do not align with the user's query and do not provide the foundational knowledge or skills related to blockchain and smart contracts. Therefore, the recommendations could be improved by focusing more on courses that are directly related to blockchain technology.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJ26SfDcPhtz"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}